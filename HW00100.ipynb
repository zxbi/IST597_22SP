{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxbi/IST597_22SP/blob/main/HW00100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHkOlMOegbJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a69b6f-b5cf-4628-f5cc-7d9e2a325303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Collecting mlxtend\n",
            "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->mlxtend) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->mlxtend) (3.1.0)\n",
            "Installing collected packages: mlxtend\n",
            "  Attempting uninstall: mlxtend\n",
            "    Found existing installation: mlxtend 0.14.0\n",
            "    Uninstalling mlxtend-0.14.0:\n",
            "      Successfully uninstalled mlxtend-0.14.0\n",
            "Successfully installed mlxtend-0.19.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "%pip install mlxtend --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXBXzFhgbKB"
      },
      "source": [
        "Let's configure all random numbers generators to support determinism and obtain reproducible results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing parameters"
      ],
      "metadata": {
        "id": "LPIjC2Ve7TiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_p=0.1\n",
        "seed=2222\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "size_input = 28*28\n",
        "size_hidden_1 = 256\n",
        "size_hidden_2 = 128\n",
        "size_output = 10\n",
        "batch_size=30\n",
        "lr=0.001\n",
        "dropout_p=0.1\n",
        "L1=0\n",
        "L2=3e-5"
      ],
      "metadata": {
        "id": "ufIFCLNJ7S7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2HEQ8zsgbKo"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "Keras comes with the MNIST data loader. Keras has a function `mnist.load_data()` which downloads the data from its servers if it is not present already. The data loaded using this function is divided into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuM3zct0gbK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd976e8-ce06-4d12-c0b3-35bf27206f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "#fashion_mnist = tf.keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GX2TaP3gbLF"
      },
      "source": [
        "# Checkout the data\n",
        "\n",
        "The data consists of handwritten numbers ranging from 0 to 9, along with their ground truth. It has 60,000 train samples and 10,000 test samples. Each sample is a 28x28 grayscale image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVCGodszgbLL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "1aNQtXQhgbLv",
        "outputId": "977cdbbd-0e38-4bcd-9dd1-3ee40a025d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape :  (60000, 28, 28) (60000,)\n",
            "Testing data shape :  (10000, 28, 28) (10000,)\n",
            "Total number of outputs :  10\n",
            "Output classes :  [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Ground Truth : 7')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print('Training data shape : ', train_images.shape, train_labels.shape)\n",
        "\n",
        "print('Testing data shape : ', test_images.shape, test_labels.shape)\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "classes = np.unique(train_labels)\n",
        "classes_num = len(classes)\n",
        "print('Total number of outputs : ', classes_num)\n",
        "print('Output classes : ', classes)\n",
        "\n",
        "plt.figure(figsize=[10,5])\n",
        "\n",
        "# Display the first image in training data\n",
        "plt.subplot(121)\n",
        "plt.imshow(train_images[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(train_labels[0]))\n",
        "\n",
        "# Display the first image in testing data\n",
        "plt.subplot(122)\n",
        "plt.imshow(test_images[0,:,:], cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(test_labels[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxNTudmrgbN9"
      },
      "source": [
        "# Process the data\n",
        "\n",
        "* The images are grayscale and the pixel values range from 0 to 255.\n",
        "* Convert each image matrix ( 28x28 ) to an array ( 28*28 = 784 dimenstional ) which will be fed to the network as a single feature.\n",
        "* We convert the data to float and **scale** the values between 0 to 1.\n",
        "* We also convert the labels from integer to **categorical ( one-hot ) encoding** since that is the format required by Keras to perform multiclass classification. One-hot encoding is a type of boolean representation of integer data. It converts the integer to an array of all zeros except a 1 at the index of the number. For example, using a one-hot encoding of 10 classes, the integer 5 will be encoded as 0000010000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpHSFQAcgbOE"
      },
      "outputs": [],
      "source": [
        "# Scale the data to lie between 0 to 1\n",
        "#train_data /= 255\n",
        "#test_data /= 255\n",
        "#val_data = train_data[-10000:]\n",
        "#val_labels = train_labels[-10000:]\n",
        "\n",
        "\n",
        "\n",
        "#(x_train, y_train), (x_test, y_test) = fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = (train_images, train_labels), (test_images, test_labels) \n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train=tf.reshape(x_train,[x_train.shape[0],-1])\n",
        "x_test=tf.reshape(x_test,[x_test.shape[0],-1])\n",
        "\n",
        "x_train, x_valid = tf.split(x_train,num_or_size_splits=[55000, 5000],axis=0)\n",
        "y_train, y_valid = tf.split(y_train, num_or_size_splits=[55000, 5000], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision"
      ],
      "metadata": {
        "id": "rqroN3w6KqXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t66544UUgbON"
      },
      "source": [
        "# Create the network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "   # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2,self.b3,self.b4]\n",
        "\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_pred_tf, labels= y_true_tf))\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-02, beta_1=0.9, beta_2=0.99, epsilon=1e-02)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        " \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #X_tf = X_tf/np.linalg.norm(X_tf)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "\n",
        "\n",
        "    what_2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat_2 = tf.nn.relu(what_2)\n",
        "\n",
        "    what_3 = tf.matmul(hhat_2, self.W3) + self.b3\n",
        "    hhat_3 = tf.nn.relu(what_3)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat_3, self.W4) + self.b4\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "72DgPtz7vt2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "from tensorflow.python.ops.gen_nn_ops import l2_loss\n",
        "class MLP_L2(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "   # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2,self.b3,self.b4]\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    lambd = 1e-5\n",
        "    m = y_true.shape[1]\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    l2_loss = tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.b1) + tf.nn.l2_loss(self.W2) + tf.nn.l2_loss(self.b2) + tf.nn.l2_loss(self.W3) + tf.nn.l2_loss(self.b3) + tf.nn.l2_loss(self.W4) + tf.nn.l2_loss(self.b4)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_pred_tf, labels= y_true_tf)) + 0.0005*l2_loss\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-02, beta_1=0.9, beta_2=0.99, epsilon=1e-02)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "   \n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "\n",
        "    #X_tf = X_tf/np.linalg.norm(X_tf)\n",
        "\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    #what = np.linalg.norm(tf.matmul(X_tf, self.W1) + self.b1)\n",
        "    hhat = tf.nn.relu(what)\n",
        "    #hhat = np.linalg.norm(tf.nn.relu(what))\n",
        "\n",
        "  \n",
        "    what_2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat_2 = tf.nn.relu(what_2)\n",
        "    #hhat_2 = np.linalg.norm(tf.nn.relu(what_2))\n",
        "\n",
        "    what_3 = tf.matmul(hhat_2, self.W3) + self.b3\n",
        "    hhat_3 = tf.nn.relu(what_3)\n",
        "    #hhat = np.linalg.norm(tf.nn.relu(what_3))\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat_3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output "
      ],
      "metadata": {
        "id": "WkmJESFkwDVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "from tensorflow.python.ops.gen_nn_ops import l2_loss\n",
        "class MLP_L2_vanilla(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "   # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2,self.b3,self.b4]\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    lambd = 1e-5\n",
        "    m = y_true.shape[1]\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    l2_loss = tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.b1) + tf.nn.l2_loss(self.W2) + tf.nn.l2_loss(self.b2) + tf.nn.l2_loss(self.W3) + tf.nn.l2_loss(self.b3) + tf.nn.l2_loss(self.W4) + tf.nn.l2_loss(self.b4)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_pred_tf, labels= y_true_tf)) + 0.0005*l2_loss\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    lr = 1e-4\n",
        "    dws = grads\n",
        "    return dws \n",
        "\n",
        "  def vanillasgd(self, grads,iter,lr=1e-4, beta_1=0.9,beta_2=0.999, beta_3 = 0.999987, epsilon=1e-8):\n",
        "    \n",
        "    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "    us = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "\n",
        "    updates = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "\n",
        "    counter = 0\n",
        "    for p, g, m, v, u in zip(self.variables, grads, ms, vs, us):\n",
        "      #print(type(p))\n",
        "      m_t = (beta_1 * m) + (1. - beta_1) * g\n",
        "      v_t = (beta_2 * v) + (1. - beta_2) * g * g \n",
        "      u_t = (beta_3 * u) + (1. - beta_3) * g * g* g\n",
        "          \n",
        "      m_t_hat = m_t / (1- K.pow(beta_1, iter))\n",
        "      v_t_hat = v_t / (1- K.pow(beta_2, iter))\n",
        "      u_t_hat = u_t / (1- K.pow(beta_3, iter))\n",
        "\n",
        "      p_t_hat = p - lr * m_t_hat / (K.sqrt(v_t_hat) + K.pow(u_t_hat, 1/3) * epsilon)\n",
        "\n",
        "\n",
        "      K.update(m, m_t_hat)\n",
        "      K.update(v, v_t_hat)\n",
        "      K.update(u, u_t_hat)\n",
        "\n",
        "      p = p_t_hat\n",
        "\n",
        "      updates.append(p_t_hat)\n",
        "           \n",
        "    for i in range(len(self.variables)):\n",
        "      self.variables[i].assign(updates[i])\n",
        "\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "   \n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "\n",
        "    #X_tf = X_tf/np.linalg.norm(X_tf)\n",
        "\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    #what = np.linalg.norm(tf.matmul(X_tf, self.W1) + self.b1)\n",
        "    hhat = tf.nn.relu(what)\n",
        "    #hhat = np.linalg.norm(tf.nn.relu(what))\n",
        "\n",
        "  \n",
        "    what_2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat_2 = tf.nn.relu(what_2)\n",
        "    #hhat_2 = np.linalg.norm(tf.nn.relu(what_2))\n",
        "\n",
        "    what_3 = tf.matmul(hhat_2, self.W3) + self.b3\n",
        "    hhat_3 = tf.nn.relu(what_3)\n",
        "    #hhat = np.linalg.norm(tf.nn.relu(what_3))\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat_3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output "
      ],
      "metadata": {
        "id": "G3SFKsM7Olla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP_DROPOUT(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "   # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2,self.b3,self.b4]\n",
        "\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_pred_tf, labels= y_true_tf))\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-02, beta_1=0.9, beta_2=0.99, epsilon=1e-02)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    p= 0.9\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #X_tf = X_tf/np.linalg.norm(X_tf)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "\n",
        "    hhat = tf.nn.relu(what)\n",
        "    hhat_drop = tf.nn.dropout(hhat, 0.1)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    what_2 = tf.matmul(hhat_drop, self.W2) + self.b2\n",
        "    hhat_2 = tf.nn.relu(what_2)\n",
        "    hhat_drop_2 = tf.nn.dropout(hhat_2, 0.1)\n",
        "    \n",
        "\n",
        "\n",
        "    what_3 = tf.matmul(hhat_drop_2, self.W3) + self.b3\n",
        "\n",
        "    hhat_3 = tf.nn.relu(what_3)\n",
        "    hhat_drop_3 = tf.nn.dropout(hhat_3, 0.1)\n",
        "    \n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat_drop_3, self.W4) + self.b4\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "by5o3jqIwjrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP_DROPOUT_Vanilla(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "   # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2,self.W3,self.W4,self.b1,self.b2,self.b3,self.W4]\n",
        " \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_pred_tf, labels= y_true_tf))\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    \n",
        "      #print(\"VARIABLES:\", self.variables)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    #print(\"GRADS:\", grads)\n",
        "    dws = grads\n",
        "    return dws \n",
        "\n",
        "\n",
        "\n",
        "  def vanillasgd(self, grads,iter,lr=1e-2, beta_1=0.9,beta_2=0.999, beta_3 = 0.999987, epsilon=1e-8):\n",
        "    \n",
        "    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "    us = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "\n",
        "    updates = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in self.variables]\n",
        "\n",
        "    counter = 0\n",
        "    for p, g, m, v, u in zip(self.variables, grads, ms, vs, us):\n",
        "      #print(type(p))\n",
        "      m_t = (beta_1 * m) + (1. - beta_1) * g\n",
        "      v_t = (beta_2 * v) + (1. - beta_2) * g * g \n",
        "      u_t = (beta_3 * u) + (1. - beta_3) * g * g* g\n",
        "          \n",
        "      m_t_hat = m_t / (1- K.pow(beta_1, iter))\n",
        "      v_t_hat = v_t / (1- K.pow(beta_2, iter))\n",
        "      u_t_hat = u_t / (1- K.pow(beta_3, iter))\n",
        "\n",
        "      p_t_hat = p - lr * m_t_hat / (K.sqrt(v_t_hat) + K.pow(u_t_hat, 1/3) * epsilon)\n",
        "\n",
        "\n",
        "      K.update(m, m_t_hat)\n",
        "      K.update(v, v_t_hat)\n",
        "      K.update(u, u_t_hat)\n",
        "\n",
        "      p = p_t_hat\n",
        "\n",
        "      updates.append(p_t_hat)\n",
        "\n",
        "    Wt = [a_i - b_i for a_i, b_i in zip(self.variables, updates)]\n",
        "           \n",
        "    for i in range(len(self.variables)):\n",
        "      self.variables[i].assign(Wt[i])\n",
        "      \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    p= 0.9\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    #X_tf = X_tf/np.linalg.norm(X_tf)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    what = what * np.random.binomial(1, p, size = what.shape) /p\n",
        "    hhat = tf.nn.relu(what)\n",
        "    hhat_drop = tf.nn.dropout(hhat, 0.1)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    what_2 = tf.matmul(hhat_drop, self.W2) + self.b2\n",
        "    what_2 = what_2 * np.random.binomial(1, p, size = what_2.shape)/p\n",
        "    hhat_2 = tf.nn.relu(what_2)\n",
        "    hhat_drop_2 = tf.nn.dropout(hhat_2, 0.1)\n",
        "    \n",
        "\n",
        "\n",
        "    what_3 = tf.matmul(hhat_drop_2, self.W3) + self.b3\n",
        "    what_3 = what_3 * np.random.binomial(1, p, size = what_3.shape) /p\n",
        "    hhat_3 = tf.nn.relu(what_3)\n",
        "    hhat_drop_3 = tf.nn.dropout(hhat_3, 0.1)\n",
        "    \n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat_drop_3, self.W4) + self.b4\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ],
      "metadata": {
        "id": "SWHEUbweHRl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Vanilla(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden_1, size_hidden_2, size_output, device=None):\n",
        "    super(MLP_Vanilla, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "\n",
        "    self.size_input, self.size_hidden_1, self.size_hidden_2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden_1, size_hidden_2, size_output, device\n",
        "\n",
        "    self.initial=tf.keras.initializers.he_normal(seed=seed)\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(self.initial([self.size_input, self.size_hidden_1]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(self.initial([1, self.size_hidden_1]))\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W2 = tf.Variable(self.initial([self.size_hidden_1, self.size_hidden_2]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(self.initial([1, self.size_hidden_2]))\n",
        "     # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(self.initial([self.size_hidden_2, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(self.initial([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.MLP_variables = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
        "\n",
        "    self.loss_object =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.reg_12=tf.keras.regularizers.L1L2(l1=L1, l2=L2)\n",
        "\n",
        "    self.t=0\n",
        "    self.m=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
        "    self.u=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
        "    self.v=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n",
        "    \n",
        "  def forward(self, training, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if training==1:\n",
        "      if self.device is not None:\n",
        "        with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "          self.y = self.compute_output(X)\n",
        "      else:\n",
        "        self.y = self.compute_output(X)\n",
        "    elif training==0:\n",
        "      if self.device is not None:\n",
        "        with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "          self.y = self.compute_output_test(X)\n",
        "      else:\n",
        "        self.y = self.compute_output_test(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    return self.loss_object(y_true, y_pred)+self.reg_12(self.W1)+self.reg_12(self.W2)\n",
        "\n",
        "  def loss2(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    return self.loss_object(y_true, y_pred)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(1,X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "    '''\n",
        "\n",
        "    self.t=self.t+1\n",
        "    beta_1=0.9\n",
        "    beta_2=0.999\n",
        "    beta_3=0.999987\n",
        "    ep=0.00000001\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(1,X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "\n",
        "\n",
        "    self.m=[a_i*beta_1+(1-beta_1)*b_i for a_i, b_i in zip(self.m, grads) ]\n",
        "    self.u=[a_i*beta_2+(1-beta_2)*(b_i**2) for a_i, b_i in zip(self.u, grads) ]\n",
        "    self.v=[a_i*beta_3+(1-beta_3)*(b_i**3) for a_i, b_i in zip(self.v, grads) ]\n",
        "\n",
        "    hm=[a_i/(1-(beta_1**self.t)) for a_i in self.m]\n",
        "    hu=[a_i/(1-(beta_2**self.t)) for a_i in self.u]\n",
        "    hv=[a_i/(1-(beta_3**self.t)) for a_i in self.v]\n",
        "    hv_sign=[tf.math.sign(a_i) for a_i in hv]\n",
        "    hv_abs=[tf.abs(a_i) for a_i in hv]\n",
        "\n",
        "    dws_new=[lr * a_i /(ep + tf.sqrt(b_i)+(d_i*tf.pow(c_i,1.0/3.0)*ep)) for a_i, b_i, c_i, d_i in zip(hm, hu, hv_abs,hv_sign)]\n",
        "\n",
        "    #print(type(dws[0:1]))\n",
        "    Wt = [a_i - b_i for a_i, b_i in zip(self.variables, dws_new)]\n",
        "    #print(type(Wt))\n",
        "\n",
        "    for i in range(len(self.variables)):\n",
        "      self.variables[i].assign(Wt[i])\n",
        "\n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    hhat = tf.nn.dropout(hhat, rate = dropout_p, seed = seed)\n",
        "    what_1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat_1 = tf.nn.relu(what_1)\n",
        "    hhat_1 = tf.nn.dropout(hhat_1, rate = dropout_p, seed = seed)\n",
        "    # Compute output\n",
        "    what_2 = tf.matmul(hhat_1, self.W3) + self.b3\n",
        "    #output= tf.nn.softmax(what_2)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return what_2\n",
        "\n",
        "  def compute_output_test(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    what_1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat_1 = tf.nn.relu(what_1)\n",
        "    # Compute output\n",
        "    what_2 = tf.matmul(hhat_1, self.W3) + self.b3\n",
        "    #output= tf.nn.softmax(what_2)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return what_2"
      ],
      "metadata": {
        "id": "TFLIXbeA_HZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "# Initialize model using GPU\n",
        "#mlp_on_gpu = MLP(size_input, size_hidden,size_output, device='gpu') \n",
        "#mlp_on_gpu = MLP_L2(size_input, size_hidden,size_output, device='gpu') \n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
        "\n",
        "train_loss.reset_states()\n",
        "train_accuracy.reset_states()\n",
        "val_loss.reset_states()\n",
        "val_accuracy.reset_states()\n",
        "\n",
        "train_loss_list = []\n",
        "train_accuracy_list = []\n",
        "test_loss_list = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "\n",
        "\n",
        "mlp_on_default = MLP_Vanilla(size_input, size_hidden_1, size_hidden_2, size_output)\n",
        "\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000, seed=seed).batch(batch_size)\n",
        "\n",
        "for inputs, outputs in valid_ds:\n",
        "  preds = mlp_on_default.forward(0,inputs)\n",
        "  val_loss(mlp_on_default.loss2(preds,outputs))\n",
        "  val_accuracy(outputs, preds)\n",
        "\n",
        "print(\n",
        "  f'Epoch {0}, '\n",
        "  f'Val Loss: {val_loss.result()}, '\n",
        "  f'Val Accuracy: {val_accuracy.result() * 100}'\n",
        ")\n",
        "\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  val_accuracy.reset_states()\n",
        "  \n",
        "  for inputs, outputs in train_ds:\n",
        "    mlp_on_default.backward(inputs, outputs)\n",
        "    preds = mlp_on_default.forward(0,inputs)\n",
        "    train_loss(mlp_on_default.loss2(preds,outputs))\n",
        "    train_accuracy(outputs, preds)\n",
        "\n",
        "\n",
        "  for inputs, outputs in valid_ds:\n",
        "    preds = mlp_on_default.forward(0,inputs)\n",
        "    val_loss(mlp_on_default.loss2(preds,outputs))\n",
        "    val_accuracy(outputs, preds)\n",
        "  \n",
        "  print(\n",
        "    f'Epoch {epoch + 1}, '\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "\n",
        "  )\n",
        "  train_accuracy_list.append(train_accuracy.result() * 100)\n",
        "  train_loss_list.append(train_loss.result() * 100)\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "test_loss.reset_states()\n",
        "test_accuracy.reset_states()\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_default.forward(0,inputs)\n",
        "  test_loss(mlp_on_default.loss2(preds,outputs))\n",
        "  test_accuracy(outputs, preds)\n",
        "  \n",
        "\n",
        "print(\n",
        "  f'Epoch {0}, '\n",
        "  f'Test Loss: {test_loss.result()}, '\n",
        "  f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(train_loss_list,'r',linewidth=0.5)\n",
        "plt.plot(train_accuracy_list,'b',linewidth=0.5)\n",
        "\n",
        "plt.plot(test_loss_list,'g',linewidth=0.5)\n",
        "plt.plot(test_accuracy_list,'y',linewidth=0.5)\n",
        "#plt.plot(mean3,'g',linewidth=0.5)\n",
        "plt.legend(['Train_loss','Train_acc','Test_lost','Test_acc'],fontsize=18)\n",
        "plt.xlabel('50 runs ',fontsize=16)\n",
        "plt.ylabel('Train_loss',fontsize=16)\n",
        "plt.title('Train_loss',fontsize=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "0scUTCRelWyF",
        "outputId": "226d2a14-d64b-421d-912f-7970389826ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Val Loss: 3.953172445297241, Val Accuracy: 9.5\n",
            "Epoch 1, Loss: 0.2376898229122162, Accuracy: 92.8472671508789, \n",
            "Epoch 2, Loss: 0.08831243962049484, Accuracy: 97.33999633789062, \n",
            "Epoch 3, Loss: 0.06050736457109451, Accuracy: 98.19636535644531, \n",
            "Epoch 4, Loss: 0.045863788574934006, Accuracy: 98.63091278076172, \n",
            "Epoch 5, Loss: 0.037017837166786194, Accuracy: 98.8654556274414, \n",
            "Epoch 6, Loss: 0.02964146062731743, Accuracy: 99.10726928710938, \n",
            "Epoch 7, Loss: 0.02568388544023037, Accuracy: 99.2036361694336, \n",
            "Epoch 8, Loss: 0.021974753588438034, Accuracy: 99.31272888183594, \n",
            "Epoch 9, Loss: 0.018130164593458176, Accuracy: 99.43818664550781, \n",
            "Epoch 10, Loss: 0.017864134162664413, Accuracy: 99.46908569335938, \n",
            "Epoch 11, Loss: 0.014880732633173466, Accuracy: 99.54545593261719, \n",
            "Epoch 12, Loss: 0.013863730244338512, Accuracy: 99.60726928710938, \n",
            "Epoch 13, Loss: 0.012705345638096333, Accuracy: 99.62000274658203, \n",
            "Epoch 14, Loss: 0.012731771916151047, Accuracy: 99.62000274658203, \n",
            "Epoch 15, Loss: 0.010359571315348148, Accuracy: 99.71273040771484, \n",
            "Epoch 16, Loss: 0.011373145505785942, Accuracy: 99.65999603271484, \n",
            "Epoch 17, Loss: 0.009966381825506687, Accuracy: 99.71818542480469, \n",
            "Epoch 18, Loss: 0.01018267497420311, Accuracy: 99.71273040771484, \n",
            "Epoch 19, Loss: 0.009932206012308598, Accuracy: 99.68000030517578, \n",
            "Epoch 20, Loss: 0.00994654931128025, Accuracy: 99.71818542480469, \n",
            "\n",
            "Total time taken (in seconds): 766.89\n",
            "Epoch 0, Test Loss: 0.07442890107631683, Test Accuracy: 97.95999908447266\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Train_loss')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "HW00100",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}